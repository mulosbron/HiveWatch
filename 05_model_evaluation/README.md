# Model Evaluation

This directory contains scripts and outputs for evaluating the performance of the trained YOLO model on the Bee vs Wasp dataset.

---

## Overview

### 1. `model_metrics.py`
The primary script that performs detailed analysis of model performance, generating visualizations and metrics.

### 2. `analysis/`
Contains all output visualizations and the metrics JSON file generated by the evaluation script.

---

## Purpose

The evaluation script analyzes model performance by:
1. Examining class distribution across the dataset
2. Analyzing object size distributions
3. Calculating IoU (Intersection over Union) metrics
4. Identifying error types (FP, FN, Localization)
5. Generating precision-recall curves and confusion matrices
6. Exporting performance metrics to JSON

---

## Key Outputs

### 1. Visualizations
- Dataset analysis: `class_distribution.png`
- Object sizes: `object_size_distribution.png`, `object_size_scatter.png`
- IoU metrics: `iou_distribution.png`, `class_iou_distribution.png`
- Error analysis: `error_type_analysis.png`, `confusion_matrix.png`, `normalized_confusion_matrix.png`
- PR curves: `precision_recall_curves.png`

### 2. Metrics
- `model_metrics.json` containing precision, recall, F1-scores and detailed error counts

### 3. Execution
To run the evaluation:
```bash
python model_metrics.py
```